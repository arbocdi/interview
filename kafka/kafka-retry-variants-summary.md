# Kafka ретраи: сводка вариантов (Spring Kafka) + псевдографика

Ниже — **3 основных подхода**, которые мы обсудили, и где какой лучше использовать. Я намеренно **без кода**, а с понятными схемами.

---

## Термины (коротко)

- **poll loop** — поток(и) consumer’а, который делает `poll()` и вызывает твой `@KafkaListener`.
- **heartbeat** — отдельный поток клиента Kafka, который шлёт heartbeats координатору группы.
- **rebalance** — перераспределение партиций между consumer’ами группы.
- **max.poll.interval.ms** — максимум времени между `poll()`-ами; если долго не поллить — считаешься “зависшим” и тебя ребалансит.
- **max.poll.records** — сколько сообщений забирается за один `poll()` (важно для задержек/тормозов).

---

# 1) Внутренние ретраи “в том же listener’е” через DefaultErrorHandler + BackOff (sleep)

Это вариант, который ты показал:

- Ошибка в обработке → `DefaultErrorHandler`
- Он делает задержку (BackOff), обычно **через `Thread.sleep`**
- Затем пытается обработать **то же самое сообщение** снова (в рамках того же consumer’а)

### Псевдосхема

```
Kafka -> poll() -> record -> @KafkaListener 
                               [FAIL]
                                 |
                                 v
                        DefaultErrorHandler
                                 |
                 +---------------+----------------+
                 |                                |
            BackOff wait                      retry attempt
           (обычно sleep)                 (повтор handler'а)
                 |
                 v
      (если исчерпали retries)
                 |
                 v
    recoverer (DLT/stop/rethrow)
```

### Что “реально” происходит по потокам

```
[consumer thread]  poll -> handle -> FAIL -> sleep/backoff -> retry -> ...
[heartbeat thread] -----------------heartbeats continue-------------------->
```

### Плюсы
- Самый простой: **всё в одном месте**.
- Не нужны дополнительные топики/хранилища.

### Минусы (важные)
- Во время задержки **consumer-thread занят** (даже если heartbeats идут).
- Риск ребаланса, если задержка+бработка суммарно близки к `max.poll.interval.ms`
- DefaultErrorHandler вызывает poll() перед каждым `sleep`


### Про ContainerPausingBackOffHandler (смысл)
Если задержка большая и ты не хочешь “залипать”, можно не sleep-ать, а **поставить контейнер/партиции на паузу** и **вовремя резюмить**.
* ``pause(partition)`` ставит партицию на паузу, но полл продолжают вызываться, возвращая 0 записей для партиции.

Псевдосхема:

```
FAIL -> backoff handler:
   pause(partition)
   schedule resume at T+delay
   return (не блокируем поток долгим sleep)
```

---

# 2) Ретраи через Kafka retry-топики (1m/2m/5m…) + репуш в основной топик

Здесь мы делаем так:

- Основной топик: `events`
- Retry-топики: `events.retry.1m`, `events.retry.5m`, `events.retry.30m`
- Ошибка → публикуем в следующий retry-топик
- Оттуда (когда “дозрело”) **репушим обратно в `events`**
- DLT: `events.dlt`

### Псевдосхема

```
                +--------------------+
                |     main topic     |
                |       events       |
                +---------+----------+
                          |
                          v
                   @Listener(main)
                          |
                       [FAIL]
                          |
                          v
                  route to retry topic
 events.retry.1m -> events.retry.5m -> events.retry.30m -> events.dlt
                          ^
                          |
                 (when due) republish to events
```

### Вариант “как держать задержку” (без sleep)

Самая полезная практика: **не спать**, а делать `pause/resume` в retry-консьюмере.

```
@Listener(retry-topic)
    if now < dueTime:
        pause(partition)
        schedule resume at dueTime
        return (offset не коммитим)
    else:
        republish -> events
        ack offset
```

### Плюсы
- “Всё на Kafka”: транспорт/аудит/наблюдаемость.
- Удобно сохранять **единую логику обработки** (всё снова идёт через `events`).
- Pause partition приводит к тому, что спринг будет вызывать `poll()`котрый возвращает 0 записей,
так продолжается до вызова resume.

### Минусы
- **Квантованные задержки** (ступени).
- “Ровно ко времени” сложно.
- Порядок может ломаться: одно событие ушло в долгий retry, следующее прошло раньше.

---

# 3) Внешний планировщик ретраев (Redis ZSET / Postgres) + (3.1) репуш в Kafka или (3.2) обработка напрямую

## 3.0 Redis ZSET как таймер-очередь (как это работает)

Идея: ZSET хранит элементы, отсортированные по score.

- `score = nextAttemptAt (epochMillis)`
- `member = messageId` (payload отдельно в HASH/JSON или внутри member)

Операции:

```
ZADD retries <nextAttemptAt> <messageId>      # поставить на ретрай ко времени
ZPOPMIN retries <N>                           # атомарно достать ближайшие (min score)
(если еще рано) ZADD retries score member     # вернуть назад
```

### Псевдосхема (pipeline)

```
Kafka events -> consumer -> handle 
                             FAIL
                              |
                              v
                    put into Redis ZSET (score = dueTime)
                              |
                              v
                  scheduler worker (poll/pop due tasks)
                       |                    |
                 if due -> republish     if due -> process
                     to Kafka (3.1)         directly (3.2)
```

## 3.1 Внешнее хранилище + репуш в Kafka
```
Redis due -> send back to "events" -> единый обработчик
```

**Плюсы:** единый путь обработки, Kafka остаётся “истиной” для обработки.  
**Минусы:** больше компонентов (Redis + воркер + мониторинг).

## 3.2 Внешнее хранилище без репуша
```
Redis due -> обработка воркером напрямую
```

**Плюсы:** можно полностью контролировать планирование/порядок/лимиты.  
**Минусы:** появляется “второй путь” обработки (primary из Kafka и retry из воркера),
надо синхронизировать логику, метрики, идемпотентность.

---

# Сравнение (коротко)

## Когда подходит 1) DefaultErrorHandler + sleep/backoff
- Ошибки редкие и задержки маленькие (секунды).
- Важно “просто и быстро внедрить”.
- Ты контролируешь `max.poll.interval.ms` и `max.poll.records` так, чтобы не ловить ребаланс.
- Нужно сохранить порядок сообщений.

## Когда подходит 2) Retry-топики в Kafka
- Хочется всё держать в Kafka, без внешних систем.
- Окей со ступенчатыми задержками.
- Нужен единый обработчик (особенно при репуше в основной топик).
- Не важен порядок сообщений.

## Когда подходит 3) Redis ZSET / Postgres scheduler
- Нужны **ретраи “ко времени”** (точно и гибко).
- Нужно **сохранять порядок по агрегату** (например, один aggregateId — один последовательный поток).
- Нужны лимиты/бекпрежер на репуш/обработку (rate limiting, batch, приоритеты).

---

# Про порядок сообщений (aggregate ordering) — в одном абзаце

Если порядок важен, самая частая практика:
- в Kafka ключ сообщения = `aggregateId` → все события агрегата попадают в **одну партицию**;
- ретраи стараются не “отрывать” событие агрегата от этой последовательности:
  - либо держат задержки внутри этой партиции (pause/resume),
  - либо в внешнем scheduler’е делают очередь по aggregateId (строгая последовательность).

---

# Мини-чеклист по рискам

- Не делай долгий `sleep` в consumer-thread без понимания `max.poll.interval.ms`.
- Для больших задержек лучше pause/resume (контейнер или партиция).
- Идемпотентность: `messageId`/dedup — почти всегда must-have (ребалансы и повторная доставка случаются).
- Наблюдаемость:
  - gauge размера очереди ретраев (retry topics lag или zset size)
  - counters: retries, fail, dlt
  - скорость роста DLQ / DLT — тревога
